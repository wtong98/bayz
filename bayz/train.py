"""
Trains a BPL model to generate music. To run:

$ python bayz.train

which will produce a directory 'save/' in your current working directory,
containing the trained model files. This code is adapted from the
BayesClef algorithm, available here: https://github.com/wtong98/4772-Project

author: William Tong (wlt2115@columbia.edu)
"""

import pickle
from collections import defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np

from gensim.models import Word2Vec
from gensim.models import KeyedVectors

from hmmlearn import hmm

from music21 import corpus
from music21 import interval
from music21 import note

from sklearn.mixture import BayesianGaussianMixture
from tqdm import tqdm

from .common import BayesianGaussianTypeModel

REST_WORD = 'REST'
START_WORD = 'START'
END_WORD = 'END'

save_path = Path('save/')
corpus_path = save_path / 'corpus.pk'
embedding_path = save_path / 'embedding.wv'
mixture_path = save_path / 'mixture.pk'
plot_path = save_path / 'plot.png'

hmm_path = save_path / 'hmm.pk'

def fetch_texts(cache=True) -> list:
    """
    Fetches the Bach corpus from music21.

    param cache: whether to save the model
    return: the Bach corpus transformed into a list of words. See
            https://github.com/wtong98/4772-Project for details on how the
            scores are tokenized
    """

    texts = None
    if corpus_path.exists() and cache:
        with corpus_path.open('rb') as fp:
            texts = pickle.load(fp)
    else:
        bach_bundle = corpus.search('bach', 'composer')
        scores = [metadata.parse() for metadata in bach_bundle]
        texts = list(tqdm(convert_to_texts(scores), total=len(scores)))
        
        if cache:
            with corpus_path.open('wb') as fp:
                pickle.dump(texts, fp)
        
    return texts


def convert_to_texts(scores: list, sampling_rate=0.5) -> 'generator':
    """
    Converts a list of music21 scores into sentences of words

    param scores: the scores to convert
    param sampling_rate: the interval in which to consider notes as being part
                         of the same word
    return: generator of converted scores
    """

    for score in scores:
        normalized_score = _transpose(score)
        text = _to_text(normalized_score, sampling_rate)
        yield text


def _transpose(score, target=note.Note('c')) -> 'Score':
    ky = score.analyze('key')
    home = note.Note(ky.tonicPitchNameWithCase)
    intv = interval.Interval(home, target)

    return score.transpose(intv)


def _to_text(score, sampling_rate) -> list:
    notes = score.flat.getElementsByClass(note.Note)
    hist = _bin(notes, sampling_rate)
    end = score.flat.highestOffset

    text = [START_WORD] + [_to_word(hist[i]) for i in np.arange(0, end, sampling_rate)] + [END_WORD]
    return text


def _bin(notes, sampling_rate) -> defaultdict:
    hist = defaultdict(list)

    for note in notes:
        offset = note.offset
        halt = offset + note.duration.quarterLength

        if _precise_round(offset % sampling_rate) != 0:
            offset = _precise_round(offset - (offset % sampling_rate))
        if _precise_round(halt % sampling_rate) != 0:
            halt = _precise_round(halt + (sampling_rate - halt % sampling_rate))

        while offset < halt:
            hist[offset].append(note)
            offset += sampling_rate

    return hist


def _to_word(notes) -> str:
    if len(notes) == 0:
        return REST_WORD

    ordered_notes = sorted(notes, key=lambda n: n.pitch.midi, reverse=True)
    word = '_'.join([note.name for note in ordered_notes])
    return word


def _precise_round(val, precision=10):
    return round(val * precision) / precision


def train_wv_model(texts: list, save=True) -> 'Word2Vec':
    """
    Trains a Word2Vec model on a list of converted scores

    param texts: scores converted into sentences of words
    param save: whether to save the model
    return: a trained gensim.Word2Vec model
    """

    model = Word2Vec(sentences=texts,
                     size=32,
                     min_count=1,
                     window=4,
                     workers=4,
                     sg=1)
    if save:
        model.wv.save(str(embedding_path))
    
    return model


def fit_mixture(embedding: 'Word2Vec', texts: list, save=True, plot=True) -> list:
    """
    Fits a Gaussian mixture to the score representations generated by a
    Word2Vec model, and produces a sequence of mixture ids corresponding to
    the vocabulary of the Word2Vec model

    param embedding: a gensim.Word2Vec model
    param texts: converted scores
    param save: whether to save the mixure model
    param plot: whether to plot the frequency of each mixture id

    return: list of mixture id's, corresponding to the corpus vocabulary
    """

    mixture = BayesianGaussianTypeModel(embedding, n_components=32)
    mixture.fit(texts)

    labels = mixture.predict(embedding.wv.vectors)
    if plot:
        plt.hist(labels, bins=32)
        plt.savefig(plot_path)
    
    if save:
        mixture.save_model(mixture_path)

    return labels


def texts_to_seqs(texts: list, wv: 'Word2Vec', labels: list) -> list:
    """
    Converts each text into a sequence of "types," aka mixture ids, based
    on the provided model and labels

    param text: converted scores
    param wv: gensim.Word2Vec model
    param labels: list of mixture ids corresponding to the Word2Vec model's
                  vocabulary
    
    return: a list of types (mixture id's) converted from the scores
    """

    word_to_label = {}
    vocab = wv.vocab
    for word in vocab:
        idx = vocab[word].index
        word_to_label[word] = labels[idx]

    def _text_to_seq(text):
        return np.array([[word_to_label[word]] for word in text])

    sequences = [_text_to_seq(text) for text in texts]
    return sequences


def train_hmm(sequences: list, save=True) -> hmm:
    """
    Trains a Hidden Markov Model on the provided sequences

    param sequences: training data
    param save: whether to save the model

    return: a trained HMM
    """

    type_gen_model = hmm.MultinomialHMM(n_components=16)
    lengths = [len(seq) for seq in sequences]
    sequences = np.concatenate(sequences)
    type_gen_model.fit(sequences, lengths=lengths)

    if save:
        with hmm_path.open('wb') as pickle_f: 
            pickle.dump(type_gen_model, pickle_f)
    
    return type_gen_model


if __name__ == '__main__':
    if not save_path.exists():
        save_path.mkdir()
    
    print('fetching corpus')
    texts = fetch_texts()

    print('training word2vec model')
    embedding = train_wv_model(texts)

    print('fitting mixture model')
    labels = fit_mixture(embedding, texts)

    print('training hmm')
    sequences = texts_to_seqs(texts, embedding.wv, labels)
    hmm = train_hmm(sequences)
    print('fitted weight matrix', hmm.transmat_)
    print('done!')
